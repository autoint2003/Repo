# CNA News Crawler Documentation

## Overview

The CNA (Channel News Asia) web crawler is a Python-based tool designed to scrape news articles from the Channel News Asia website. It systematically collects article metadata, content, and structural information for text mining and analysis purposes.

## Architecture

### Class Structure

The crawler is implemented as a single `CNACrawler` class with three main responsibilities:

1. **Link Discovery**: Finding article URLs from section listing pages
2. **Content Extraction**: Scraping individual articles and parsing their structure
3. **Data Export**: Saving collected articles to CSV and JSON formats

### Core Components

```
CNACrawler
├── __init__()              # Initialize crawler with base URL and headers
├── get_article_links()     # Discover article URLs from listing pages
├── scrape_article()        # Extract content and metadata from individual articles
├── crawl()                 # Main orchestration method
├── save_to_csv()          # Export data to CSV format
└── save_to_json()         # Export data to JSON format
```

## How the Crawler Works

### 1. Link Discovery Process

The `get_article_links()` method:

- **Input**: Section name (e.g., 'news', 'singapore', 'world') and maximum pages to crawl
- **Process**:
  1. Constructs URL for the section listing page
  2. Sends HTTP GET request with browser-like headers to avoid blocking
  3. Parses HTML using BeautifulSoup
  4. Identifies article links by filtering for URLs containing section keywords
  5. Converts relative URLs to absolute URLs
  6. Deduplicates links and filters out non-article pages
  7. Implements 1-second delay between page requests (rate limiting)
- **Output**: List of unique article URLs

### 2. Article Scraping Process

The `scrape_article()` method extracts comprehensive information from each article:

#### Title Extraction
- Locates the `<h1>` tag containing the article headline

#### Content Container Detection
Uses multiple fallback strategies to locate article body:
1. `div.text-long` (primary container)
2. `div.article__body` (alternative container)
3. `<article>` tag
4. Any div with 'content' in its class name

#### Advanced Paragraph Processing

The improved version implements a sophisticated multi-stage pipeline:

##### Stage 1: Initial Collection
- Extracts all `<p>` tags from the content container
- Converts to text with proper spacing

##### Stage 2: Metadata Extraction
- **Section Identification**: Detects section keywords (Singapore, World, Asia, Business, Sport) at the beginning of the first paragraph
- **Subtitle Extraction**: Captures descriptive text following the section identifier
- **Location Parsing**: Uses regex `^([A-Z][A-Z\s,]+):\s*(.+)$` to identify location prefixes (e.g., "SINGAPORE:", "TUCSON, Arizona:")

##### Stage 3: Content Cleaning
Filters out non-content elements:
- **Photo Captions**: Removes patterns like `(Photo: ...)` and `(File photo: ...)` using regex `\((File )?[Pp]hoto:\s*[^)]+\)`
- **AI Disclaimers**: Filters out "This audio is generated by an AI tool" notices
- **Newsletter Prompts**: Detects and stops processing when encountering subscription-related keywords:
  - "Get our pick of top stories"
  - "Stay updated with notifications"
  - "Get WhatsApp alerts"
  - "Join our channel for the top reads"
  - "Subscribe to"
  - E.U. service disclaimers
  - "By clicking subscribe"

##### Stage 4: Location Prefix Handling
- Detects location prefixes at the start of paragraphs
- Stores location metadata separately
- Preserves remaining paragraph content after the location prefix

#### Date Extraction
Multiple fallback strategies:
1. `<time>` tag with `datetime` attribute
2. `<time>` tag text content
3. Elements with 'date' in class name

#### Category Detection
Hierarchical approach:
1. Breadcrumb navigation (most reliable)
2. URL path analysis
3. Section metadata fallback

### 3. Main Crawling Flow

The `crawl()` method orchestrates the entire process:

```
1. Call get_article_links() to discover URLs
2. Limit to max_articles if specified
3. For each article URL:
   a. Call scrape_article()
   b. Validate that body_content exists
   c. Append to articles list
   d. Display progress with location info
   e. Wait 2 seconds (rate limiting)
4. Return collected articles
```

### 4. Data Export

#### CSV Export (`save_to_csv()`)
- Converts articles list to pandas DataFrame
- Saves with UTF-8 encoding
- Preserves all metadata fields

#### JSON Export (`save_to_json()`)
- Exports as formatted JSON with 2-space indentation
- Maintains non-ASCII characters (ensure_ascii=False)
- Human-readable structure

## Data Schema

Each scraped article contains the following fields:

| Field | Type | Description |
|-------|------|-------------|
| `url` | string | Full URL of the article |
| `title` | string | Article headline |
| `section` | string | Content section (Singapore, World, etc.) |
| `subtitle` | string | Article subtitle or standfirst |
| `location` | string | Geographic location identifier |
| `body_content` | string | Clean article text (paragraphs joined) |
| `publication_date` | string | Original publication date/time |
| `category` | string | Article category from breadcrumb/URL |
| `scraped_at` | string | ISO timestamp of scraping |

## Improvements Made

### Version 1 → Version 2 (Enhanced Parser)

#### 1. Enhanced Metadata Extraction

**Original**:
```python
body_content = [p.get_text(strip=True) for p in paragraphs]
```

**Improved**:
- Extracts `section`, `subtitle`, and `location` as separate structured fields
- Implements intelligent first-paragraph parsing
- Detects location prefixes with regex patterns

**Impact**: Enables better article categorization and geographic analysis

#### 2. Content Quality Improvements

**New Filtering Mechanisms**:
- **Photo Caption Removal**: Regex-based detection and removal of image credits
- **AI Disclaimer Filtering**: Removes automated audio notices
- **Newsletter Detection**: Stops processing when subscription prompts appear
- **Empty Paragraph Handling**: Skips paragraphs that become empty after cleaning

**Impact**: Produces cleaner text for downstream NLP tasks, reducing noise in the corpus

#### 3. Location Intelligence

**Pattern Recognition**:
```python
location_match = re.match(r'^([A-Z][A-Z\s,]+):\s*(.+)$', para)
```

**Functionality**:
- Detects dateline-style location prefixes
- Separates location metadata from content
- Preserves remaining paragraph text

**Impact**: Enables geographic filtering and location-based analysis

#### 4. Better Section Handling

**Section Keywords Detection**:
```python
section_keywords = ['Singapore', 'World', 'Asia', 'Business', 'Sport']
```

**Process**:
- Identifies section at the start of first paragraph
- Extracts subtitle by removing section prefix
- Falls back to category detection from URL/breadcrumb

**Impact**: More reliable article categorization

#### 5. Improved Output and Debugging

**Enhanced Progress Display**:
```python
loc_info = f" [{article['location']}]" if article['location'] else ""
print(f"  ✓ Scraped: {article['title'][:50]}...{loc_info}")
```

**Sample Output Enhancement**:
- Shows section, subtitle, and location in sample display
- Better formatting for long subtitles

**Impact**: Better visibility into crawling progress and data quality

## Usage Example

```python
from crawl_cna import CNACrawler

# Initialize crawler
crawler = CNACrawler()

# Crawl articles
articles = crawler.crawl(
    section='singapore',  # Target section
    max_pages=5,          # Crawl 5 listing pages
    max_articles=50       # Limit to 50 articles
)

# Export data
crawler.save_to_csv('singapore_news.csv')
crawler.save_to_json('singapore_news.json')

# Access article data
for article in articles:
    print(f"{article['title']} - {article['location']}")
```

## Best Practices

### Rate Limiting
- 1-second delay between page requests
- 2-second delay between article scrapes
- Prevents server overload and reduces blocking risk

### Error Handling
- Try-except blocks around all HTTP requests
- Graceful degradation when elements are missing
- Continues crawling even if individual articles fail

### Respectful Crawling
- Custom User-Agent header to identify the crawler
- Reasonable timeout values (10 seconds)
- Rate limiting to avoid overwhelming the server

## Technical Dependencies

```python
requests==2.31.0       # HTTP client
beautifulsoup4==4.12.0 # HTML parsing
pandas==2.0.0          # Data manipulation
```

## Limitations and Considerations

1. **Website Structure Dependency**: Crawler relies on CNA's HTML structure; changes to the website may require updates
2. **JavaScript Content**: Does not execute JavaScript; only captures server-rendered content
3. **Rate Limiting**: Conservative delays mean large-scale crawling takes time
4. **Dynamic Content**: Newsletter prompts and other dynamic elements may vary by user session

## Future Enhancements

Potential improvements for future versions:

1. **Parallel Processing**: Implement concurrent article scraping with thread/process pools
2. **Resume Capability**: Save progress to allow resuming interrupted crawls
3. **Author Extraction**: Add logic to capture article authors
4. **Image Metadata**: Extract and store image URLs and captions
5. **Comment Scraping**: Optionally capture user comments
6. **Incremental Updates**: Check for already-scraped URLs to avoid duplicates
7. **Selenium Integration**: Handle JavaScript-rendered content
8. **Error Recovery**: Implement retry logic with exponential backoff

## Conclusion

The CNA crawler has evolved from a basic scraper to a sophisticated content extraction tool with intelligent parsing, metadata enrichment, and content cleaning capabilities. The improvements focus on data quality, enabling more accurate text mining and analysis while maintaining respectful crawling practices.
